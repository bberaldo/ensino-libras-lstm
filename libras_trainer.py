# ========= In√≠cio =========
import os, sys, traceback, faulthandler, threading, numpy as np
from PIL import ImageFont, ImageDraw, Image

# Habilita o rastreamento de falhas em todas as threads
# √ötil para debug de crashes que n√£o geram exce√ß√µes Python normais
faulthandler.enable(all_threads=True)

# Hook personalizado para capturar exce√ß√µes "n√£o-levant√°veis"
# (ex: erros em __del__, callbacks de threads)
def _unraisable_hook(unraisable):
    print("UNRAISABLE:", unraisable.exc_type, unraisable.exc_value, "in", unraisable.object, file=sys.stderr)
sys.unraisablehook = _unraisable_hook

# Hook para capturar exce√ß√µes em threads secund√°rias
def _thread_excepthook(args):
    traceback.print_exception(args.exc_type, args.exc_value, args.exc_traceback)
threading.excepthook = _thread_excepthook

# Reduz logs verbosos do TensorFlow (apenas erros cr√≠ticos)
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
# Desabilita transforma√ß√µes de hardware do OpenCV no Windows
# Aumenta estabilidade da captura de v√≠deo
os.environ.setdefault("OPENCV_VIDEOIO_MSMF_ENABLE_HW_TRANSFORMS", "0")

# Silencia logs do OpenCV
os.environ.setdefault("OPENCV_LOG_LEVEL", "SILENT")

# Hook adicional para garantir que exce√ß√µes sejam registradas mesmo em callbacks Tkinter
def _plain_excepthook(exc_type, exc, tb):
    import traceback, sys
    try:
        traceback.print_exception(exc_type, exc, tb, file=sys.__stderr__)
    except Exception as ee:
        try:
            sys.__stderr__.write(f"excepthook falhou: {ee!r}\n")
        except:
            pass
sys.excepthook = _plain_excepthook

print("[BOOT] after hooks/env (Tk paths set)", flush=True)

# --- Interface Gr√°fica ---
import tkinter as tk
from tkinter import ttk
from collections import deque # Estrutura de dados para buffer circular

import cv2
cv2.setNumThreads(1)  # For√ßa uso de apenas 1 thread para maior estabilidade no Windows

# --- Machine Learning ---
import tensorflow as tf

print("[BOOT] libs importadas", flush=True)

# =========================
# Configura√ß√µes do modelo
# =========================
# Caminhos dos arquivos do modelo treinado
MODEL_PATHS = ["checkpoints/final_model-v2.keras"]
CLASSES_PATH = "classes.npy"

# PAR√ÇMETROS DA LSTM
SEQLEN = 30 # Tamanho da sequ√™ncia temporal (30 frames por gesto)

# PAR√ÇMETROS DE SUAVIZA√á√ÉO E CONFIAN√áA
SMOOTH_K = 5  # Janela de suaviza√ß√£o temporal (m√©dia dos √∫ltimos 5 frames)
CONF_THRESH = 0.95 # Threshold de confian√ßa m√≠nima para aceitar uma predi√ß√£o (95%)

# PAR√ÇMETROS DE DETEC√á√ÉO DE MOVIMENTO
MIN_LANDMARKS = 1  # M√≠nimo de landmarks detectados (pelo menos 1 m√£o vis√≠vel)
MOTION_EPS    = 5e-4  # Epsilon para detectar movimento (evita reconhecer gestos parados)
MOTION_MIN_FRAMES = 6 # N√∫mero m√≠nimo de frames para calcular movimento

# PAR√ÇMETROS DE VALIDA√á√ÉO DE PREDI√á√ïES
MARGIN_THRESH = 0.30 # Margem m√≠nima entre a classe mais prov√°vel e a segunda (30%)
ENTROPY_MAX   = 1.0 # Entropia m√°xima permitida (mede incerteza da predi√ß√£o)

# PAR√ÇMETROS DE ACEITA√á√ÉO DE GESTOS
TARGET_ACCEPT = 0.80 # Acur√°cia m√≠nima para aceitar um gesto (80%)
TARGET_STREAK = 8 # N√∫mero consecutivo de frames com acur√°cia >= 80% (evita falsos positivos)

# Lista de classes/gestos que o modelo reconhece
actions = np.array([
    'bom-bem', 'dia', 'oi', 'joia', 'eu', 'amo',
    'voce', 'obrigado', 'desculpa', 'pessoa', 'brasil'
])
np.save(CLASSES_PATH, actions)

# =========================
# Fases do curso 
# =========================
PHASES = [
    {"title": "1) Oi, tudo bem",      "sequence": ['oi', 'bom-bem', 'joia'], "phrase": "‚ÄúOi, tudo bem?‚Äù",     "video": "assets/oi_tudo_bem.mp4"},
    {"title": "2) Eu sou brasileiro", "sequence": ['pessoa', 'brasil'],      "phrase": "‚ÄúEu sou brasileiro‚Äù", "video": "assets/eu_sou_brasileiro.mp4"},
    {"title": "3) Obrigado",          "sequence": ['obrigado'],              "phrase": "‚ÄúObrigado(a)‚Äù",       "video": "assets/obrigado.mp4"},
    {"title": "4) Eu amo voc√™",       "sequence": ['eu', 'amo', 'voce'],     "phrase": "‚ÄúEu amo voc√™‚Äù",       "video": "assets/eu_amo_voce.mp4"},
    {"title": "5) Desculpa",          "sequence": ['desculpa'],              "phrase": "‚ÄúDesculpa‚Äù",          "video": "assets/desculpa.mp4"},
    {"title": "6) Bom dia",           "sequence": ['bom-bem', 'dia'],        "phrase": "‚ÄúBom dia‚Äù",           "video": "assets/bom_dia.mp4"},
]

# =========================
# Utilit√°rios
# =========================
def load_model_and_classes():
    classes = np.load(CLASSES_PATH, allow_pickle=True)
    model = None

    # Tenta carregar de cada caminho at√© conseguir
    for p in MODEL_PATHS:
        try:
            model = tf.keras.models.load_model(p)
            print(f"[OK] Modelo carregado: {p}", flush=True)
            break
        except Exception as e:
            print(f"[!] N√£o foi poss√≠vel carregar {p}: {e}", flush=True)
    if model is None:
        raise RuntimeError("Nenhum modelo foi carregado. Verifique os caminhos.")
    return model, classes

def count_landmarks(results):
    # Conta o num total de landmarks detectados pelo MediaPipe
    # Verifica apenas as m√£os (esquerda e direita), pois s√£o essenciais para LIBRAS
    c = 0
    if results.left_hand_landmarks and results.left_hand_landmarks.landmark:
        c += len(results.left_hand_landmarks.landmark)
    if results.right_hand_landmarks and results.right_hand_landmarks.landmark:
        c += len(results.right_hand_landmarks.landmark)
    return c

def motion_energy_last(seq_buf, k=6):
    # calcula a energia de movimentos nos √∫ltimos K frames
    # A energia de movimento mede o quanto os landmarks se moveram entre frames.
    # Valores baixos indicam que a pessoa est√° parada (gesto n√£o est√° sendo feito).
    n = min(len(seq_buf), k)
    if n < 2: return 0.0

    # Empilha os √∫ltimos n frames em um array 3D
    x = np.stack(list(seq_buf)[-n:], axis=0)

    # Calcula diferen√ßa entre frames consecutivos
    dx = np.diff(x, axis=0)

    # Retorna a norma m√©dia (magnitude do movimento)
    return float(np.mean(np.linalg.norm(dx, axis=1)))

def motion_energy(seq_buf):
    # Calcula a energia de movimento em toda a sequ√™ncia do buffer, similar a motion_energy_last, mas considera todos os frames dispon√≠veis
    if len(seq_buf) < 2: return 0.0
    x = np.stack(seq_buf, axis=0)
    dx = np.diff(x, axis=0)
    e = np.mean(np.linalg.norm(dx, axis=1))
    return float(e)

def entropy(p):
    # Calcula a entropia de Shannon de uma distribui√ß√£o de probabilidade
    # Entropia mede a "incerteza" da predi√ß√£o:
    # Entropia baixa: modelo confiante (ex: [0.95, 0.03, 0.02] ‚Üí entropia ~0.3)
    # Entropia alta: modelo confuso (ex: [0.4, 0.35, 0.25] ‚Üí entropia ~1.1)
    p = np.clip(p, 1e-9, 1.0)
    return float(-np.sum(p * np.log(p)))

def should_abstain(p, conf_thresh=0.99):
    # Decide se o modelo deve se abster de fazer uma predi√ß√£o
    maxp = float(np.max(p))

    # Crit√©rio 1: Confian√ßa abaixo do threshold
    if maxp < conf_thresh:
        return True
    
    # Crit√©rio 2: Margem insuficiente entre top-2 classes
    sorted_p = np.sort(p)[::-1]
    margin = float(sorted_p[0] - (sorted_p[1] if len(sorted_p) > 1 else 0.0))
    if margin < MARGIN_THRESH:
        return True
    
    # Crit√©rio 3: Entropia muito alta (modelo confuso)
    if entropy(p) > ENTROPY_MAX:
        return True
    return False

# Vari√°veis globais para modelo e classes
# (carregadas posteriormente na inicializa√ß√£o da GUI)
model = None
classes = np.load(CLASSES_PATH, allow_pickle=True)

def build_keep_idx():
    # Constr√≥i os √≠ndices dos landmarks relevantes para LIBRAS
    # MediaPipe Holistic retorna 1662 valores, mas usamos apenas 144:
    # - 6 pontos da POSE (ombros, cotovelos, punhos) √ó 3 coords √ó 1 visibility = 18
    # - 21 pontos da m√£o esquerda √ó 3 coords = 63
    # - 21 pontos da m√£o direita √ó 3 coords = 63
    # Total: 18 + 63 + 63 = 144 features

    # Offsets de cada regi√£o no vetor completo
    pose_offset  = 0
    face_offset  = 33 * 4
    handL_offset = face_offset + 468 * 3
    handR_offset = handL_offset + 21 * 3

    # √çndices dos pontos da pose que queremos (parte superior do corpo)
    # 11-12: ombros, 13-14: cotovelos, 15-16: pulsos
    pose_keep = np.array([11, 12, 13, 14, 15, 16])

    # Expande para pegar [x, y, z] de cada ponto (ignora visibility aqui)
    idx_pose_sup = (pose_keep[:, None] * 4 + np.array([0, 1, 2])).reshape(-1)

    # √çndices de todas as coordenadas das m√£os
    idx_handL = np.arange(handL_offset, handL_offset + 21 * 3)
    idx_handR = np.arange(handR_offset, handR_offset + 21 * 3)

    # Concatena todos os √≠ndices relevantes
    keep_idx = np.concatenate([idx_pose_sup, idx_handL, idx_handR])
    return keep_idx

# Gera os √≠ndices uma vez no in√≠cio
KEEP_IDX = build_keep_idx()
D0 = KEEP_IDX.size  # 144 features

def apply_keep_idx_feat(feat1662):
    # Extrai apenas as 144 features relevantes do vetor completo de 1662
    return feat1662[KEEP_IDX]

class TemporalSmoother:
    # Classe para suaviza√ß√£o temporal das predi√ß√µes.
    
    # Mant√©m um buffer circular com as √∫ltimas K predi√ß√µes e retorna a m√©dia.  Isso reduz "ru√≠do" nas predi√ß√µes frame-a-frame, tornando o reconhecimento mais est√°vel e confi√°vel.

    def __init__(self, k=5, num_classes=None):
        self.k = k
        self.buf = deque(maxlen=k)
        self.num_classes = num_classes
    def push(self, probs):
        # Adiciona uma nova predi√ß√£o e retorna a m√©dia suavizada
        self.buf.append(probs)

        # Calcula m√©dia das √∫ltimas K predi√ß√µes
        avg = np.mean(self.buf, axis=0)

        # Classe com maior probabilidade m√©dia
        cls = int(np.argmax(avg))

        # Confian√ßa da classe escolhida
        conf = float(np.max(avg))

        return avg, cls, conf
    def clear(self):
        # Limpa o buffer (usado ao resetar ou trocar de fase).
        self.buf.clear()

# =========================
# MediaPipe
# =========================

# MediaPipe √© inicializado apenas quando necess√°rio (modo lazy loading)
# para economizar recursos e acelerar a inicializa√ß√£o da aplica√ß√£o
mp_holistic = None 

def extract_features_holistic(results):
    # Extrai features de um frame processado pelo MediaPipe Holistic.
    
    # O MediaPipe retorna landmarks de:
    # - Pose (33 pontos do corpo)
    # - Face (468 pontos faciais)
    # - M√£os esquerda e direita (21 pontos cada)

    # coordenadas (x, y, z)

    def flatten_landmarks(landmarks, include_visibility=False):
        # Converte lista de landmarks em array numpy flat
        if landmarks is None:
            return None
        out = []
        for lm in landmarks:
            x = lm.x  # Coordenada horizontal [0, 1]
            y = lm.y  # Coordenada vertical [0, 1]
            z = lm.z  # Profundidade (dist√¢ncia da c√¢mera)

            if include_visibility:
                # Visibility indica confian√ßa da detec√ß√£o [0, 1]
                v = getattr(lm, "visibility", 0.0)
                out.extend([x, y, z, v])
            else:
                out.extend([x, y, z])
        return np.array(out, dtype=np.float32)

    # ---- Extra√ß√£o da POSE (33 pontos √ó 4 = 132 valores) ----
    if results.pose_landmarks and results.pose_landmarks.landmark:
        pose = flatten_landmarks(results.pose_landmarks.landmark, include_visibility=True)
    else:
        # Se n√£o detectou pose, preenche com zeros
        pose = np.zeros(33 * 4, dtype=np.float32)

    # ---- Extra√ß√£o da FACE (468 pontos √ó 3 = 1404 valores) ----
    if results.face_landmarks and results.face_landmarks.landmark:
        face = flatten_landmarks(results.face_landmarks.landmark, include_visibility=False)
    else:
        face = np.zeros(468 * 3, dtype=np.float32)

    # ---- Extra√ß√£o da M√ÉO ESQUERDA (21 pontos √ó 3 = 63 valores) ----
    if results.left_hand_landmarks and results.left_hand_landmarks.landmark:
        lh = flatten_landmarks(results.left_hand_landmarks.landmark, include_visibility=False)
    else:
        lh = np.zeros(21 * 3, dtype=np.float32)

    # ---- Extra√ß√£o da M√ÉO DIREITA (21 pontos √ó 3 = 63 valores) ----
    if results.right_hand_landmarks and results.right_hand_landmarks.landmark:
        rh = flatten_landmarks(results.right_hand_landmarks.landmark, include_visibility=False)
    else:
        rh = np.zeros(21 * 3, dtype=np.float32)

    # Concatena tudo em um √∫nico vetor de features
    feat = np.concatenate([pose, face, lh, rh], axis=0)

    # Valida√ß√£o: deve ter exatamente 1662 valores
    assert feat.shape[0] == 1662, f"Esperado 1662, obtido {feat.shape[0]}"
    return feat

# =========================
# Tkinter GUI com Fases
# =========================
class LibrasFasesGUI(tk.Tk):
    # Implementa um sistema de fases para ensino progressivo de LIBRAS.
    # Cada fase cont√©m uma sequ√™ncia de gestos que o usu√°rio deve executar.
    
    # Estados da aplica√ß√£o:
    # - INTRO: Tela inicial com v√≠deo demonstrativo
    # - DETECT: Modo de detec√ß√£o com webcam ativa
    # - DONE: Tela de conclus√£o de fase
    # - FINAL: Tela de conclus√£o do curso completo

    def __init__(self):
        # Inicializa a interface gr√°fica e todos os componentes
        print("[TK] criando janela", flush=True)
        super().__init__()

        # ---- Configura√ß√£o da janela principal ----
        self.title("Treinador de LIBRAS ‚Äî Fases")
        self.protocol("WM_DELETE_WINDOW", self.on_close)
        self.geometry("1024x720")

        # Captura exce√ß√µes de callbacks do Tkinter
        self.report_callback_exception = self._report_callback_exception

        # ------- Estado da aplica√ß√£o -------
        self.phases = PHASES
        self.phase_idx = 0      # Fase atual (0 = primeira fase)
        self.step_idx = 0       # Gesto atual dentro da fase
        self.target_streak = 0  # Quantos frames seguidos acertou o gesto
        self.state = "INTRO"    # Estado inicial: tela de introdu√ß√£o

        # ------- Carregamento do modelo LSTM -------
        global model, classes
        if model is None:
            print("[BOOT] carregando modelo/classes...", flush=True)
            model, classes = load_model_and_classes()

            # Valida√ß√£o: n√∫mero de sa√≠das do modelo deve bater com n√∫mero de classes
            assert model.output_shape[-1] == len(classes), "n√∫mero de sa√≠das do modelo ‚â† n¬∫ de classes"
            print("Ordem das classes:", list(classes), flush=True)
        self.model = model
        self.classes = classes

        # ---- Inicializa√ß√£o de captura de v√≠deo ----
        # Webcam s√≥ √© aberta quando entrar no modo DETECT (economiza recursos)
        self.cap = None
        self.holistic = None

        # ---- Buffers para processamento temporal ----
        self.seq_buf = deque(maxlen=SEQLEN)  # Buffer circular com √∫ltimos 30 frames
        self.smoother = TemporalSmoother(k=SMOOTH_K, num_classes=len(self.classes))
        self.last_avg = None  # √öltima predi√ß√£o m√©dia (para debugging)

        # ---- Container principal ----
        self.container = ttk.Frame(self)
        self.container.pack(fill="both", expand=True)

        # Configura√ß√£o do grid: linha 0 = header fixo, linha 1 = conte√∫do expans√≠vel
        self.container.grid_rowconfigure(0, weight=0)  # Header n√£o expande
        self.container.grid_rowconfigure(1, weight=1)  # Conte√∫do expande
        self.container.grid_columnconfigure(0, weight=1)

        # ---- Header (t√≠tulo e status) ----
        self.header = ttk.Frame(self.container)
        self.header.grid(row=0, column=0, sticky="ew", padx=12, pady=(12, 6))

        # Label do t√≠tulo da fase (esquerda)
        self.lbl_title = ttk.Label(self.header, text="", font=("Arial", 18, "bold"))
        self.lbl_title.pack(side="left")

        # Label de status (direita)
        self.lbl_status = ttk.Label(self.header, text="", foreground="#0a84ff", font=("Arial", 12, "bold"))
        self.lbl_status.pack(side="right")

        # ---- Frames "telas" empilhados ----
        # Todas as telas ocupam o mesmo espa√ßo (linha 1 do grid)
        # Usamos tkraise() para mostrar apenas uma por vez
        self.frame_intro  = ttk.Frame(self.container)
        self.frame_detect = ttk.Frame(self.container)
        self.frame_done   = ttk.Frame(self.container)

        for f in (self.frame_intro, self.frame_detect, self.frame_done):
            f.grid(row=1, column=0, sticky="nsew")

        # --- INTRO (V√≠deo demonstrativo) ---
        self.intro_center = ttk.Frame(self.frame_intro)
        self.intro_center.place(relx=0.5, rely=0.5, anchor="center")

        # Label com a frase em portugu√™s
        self.lbl_phrase   = ttk.Label(self.intro_center, text="", font=("Arial", 16))
        self.lbl_phrase.pack(pady=(0,10))

        # Label para exibir o v√≠deo tutorial
        self.preview_label = tk.Label(self.intro_center, bg="black", width=900, height=506)
        self.preview_label.pack()

        # Dica de instru√ß√£o
        self.hint_label = ttk.Label(
            self.intro_center,
            text="Assista ao v√≠deo e, quando estiver pronto, clique em ‚ÄúEstou pronto‚Äù.",
            font=("Arial", 12)
        )
        self.hint_label.pack(pady=(10,12))

        # Bot√£o para iniciar detec√ß√£o
        style = ttk.Style()
        style.configure('Ready.TButton', font=('Arial', 14, 'bold'), padding=15)
        
        self.btn_ready = ttk.Button(
            self.intro_center, 
            text="‚úì Estou pronto", 
            command=self.start_detect,
            style='Ready.TButton'
        )
        self.btn_ready.pack(pady=10)

        # --- DETECT (Detec√ß√£o em tempo real) ---
        # Label para exibir o feed da webcam
        self.video_label = tk.Label(self.frame_detect, bg="black")
        self.video_label.place(relx=0.5, rely=0.5, anchor="center")

        # Controles na parte inferior
        self.controls_detect = ttk.Frame(self.frame_detect)
        self.controls_detect.pack(side="bottom", pady=20)
        style = ttk.Style()
        style.configure('Large.TButton', font=('Arial', 11), padding=10)
        
        # Bot√£o: Reiniciar fase
        ttk.Button(
            self.controls_detect, 
            text="üîÑ Reiniciar fase", 
            command=self.reset_phase,
            style='Large.TButton'
        ).pack(side="left", padx=8)
        
        # Bot√£o: Fase anterior (s√≥ aparece se n√£o for a primeira fase)
        self.btn_prev_phase = ttk.Button(
            self.controls_detect, 
            text="‚¨Ö Fase anterior", 
            command=self.prev_phase,
            style='Large.TButton'
        )
        self.btn_prev_phase.pack(side="left", padx=8)
        
        # Bot√£o: Voltar √† introdu√ß√£o (volta para o v√≠deo demonstrativo)
        ttk.Button(
            self.controls_detect, 
            text="üè† Voltar √† introdu√ß√£o", 
            command=self.back_to_intro,
            style='Large.TButton'
        ).pack(side="left", padx=8)

        # --- DONE (Fase conclu√≠da) ---
        self.done_center = ttk.Frame(self.frame_done)
        self.done_center.place(relx=0.5, rely=0.5, anchor="center")

        # T√≠tulo de conclus√£o
        self.lbl_done = ttk.Label(self.done_center, text="üéâ Fase conclu√≠da!", font=("Arial", 20, "bold"))
        self.lbl_done.pack(pady=(0,10))

        # Subt√≠tulo com lista de gestos executados
        self.lbl_done_sub = ttk.Label(self.done_center, text="", font=("Arial", 12))
        self.lbl_done_sub.pack(pady=(0,16))

        # Container de bot√µes
        btns = ttk.Frame(self.done_center)
        btns.pack()
        
        style = ttk.Style()
        style.configure('Action.TButton', font=('Arial', 12), padding=12)
        
        # Bot√£o: Repetir fase atual
        ttk.Button(
            btns, 
            text="üîÅ Repetir esta fase", 
            command=self.reset_phase,
            style='Action.TButton'
        ).pack(side="left", padx=10)
        
        # Bot√£o: Avan√ßar para pr√≥xima fase
        ttk.Button(
            btns, 
            text="‚û° Pr√≥xima fase", 
            command=self.next_phase,
            style='Action.TButton'
        ).pack(side="left", padx=10)
        
        # Bot√£o: Sair da aplica√ß√£o
        ttk.Button(
            btns, 
            text="‚ùå Sair", 
            command=self.on_close,
            style='Action.TButton'
        ).pack(side="left", padx=10)

        # --- FINAL (Todas as fases conclu√≠das) ---
        self.frame_final = ttk.Frame(self.container)
        self.frame_final.grid(row=1, column=0, sticky="nsew")
        
        # Layout centralizado
        self.final_center = ttk.Frame(self.frame_final)
        self.final_center.place(relx=0.5, rely=0.5, anchor="center")
        
        # T√≠tulo de parab√©ns
        self.lbl_final_title = ttk.Label(
            self.final_center, 
            text="üéä PARAB√âNS! üéä", 
            font=("Arial", 28, "bold"),
            foreground="#00aa00"
        )
        self.lbl_final_title.pack(pady=(0,20))
        
        # Mensagem de conclus√£o do curso
        self.lbl_final_msg = ttk.Label(
            self.final_center,
            text="Voc√™ completou todas as fases do curso de LIBRAS!\n\nContinue praticando para melhorar ainda mais.",
            font=("Arial", 14),
            justify="center"
        )
        self.lbl_final_msg.pack(pady=(0,30))
        
        # Bot√µes finais
        final_btns = ttk.Frame(self.final_center)
        final_btns.pack()
        
        style = ttk.Style()
        style.configure('Final.TButton', font=('Arial', 13, 'bold'), padding=15)
        
        # Bot√£o: Recome√ßar curso do zero
        ttk.Button(
            final_btns, 
            text="üîÑ Recome√ßar do in√≠cio", 
            command=self.restart_course,
            style='Final.TButton'
        ).pack(side="left", padx=12)
        
        # Bot√£o: Sair da aplica√ß√£o
        ttk.Button(
            final_btns, 
            text="üëã Sair", 
            command=self.on_close,
            style='Final.TButton'
        ).pack(side="left", padx=12)

        # ---- Vari√°veis do reprodutor de v√≠deo tutorial ----
        self.tutorial_cap = None        # Objeto VideoCapture do v√≠deo tutorial
        self.tutorial_path = None       # Caminho do v√≠deo atual
        self.tutorial_running = False   # Flag indicando se o v√≠deo est√° rodando

        # ---- Configura√ß√µes de exibi√ß√£o ----
        self.video_w_target = 960  # Largura alvo para redimensionamento do v√≠deo

        # ---- Inicializa√ß√£o final ----
        self.update_phase_labels()      # Atualiza labels com informa√ß√µes da fase atual
        self.show_state("INTRO")        # Inicia no estado INTRO
        print("[STATE] INTRO", flush=True)

        # Inicia o loop principal ap√≥s 10ms (permite que a GUI seja montada primeiro)
        self.after(10, self.main_loop)

    # ------- M√©todos auxiliares da interface -------
    def _report_callback_exception(self, exc, val, tb):
        # Hook personalizado para capturar exce√ß√µes em callbacks do Tkinter.
        
        # Tkinter por padr√£o apenas imprime exce√ß√µes no console. Este m√©todo garante que exce√ß√µes sejam registradas em arquivo para debugging.

        # Imprime no console
        traceback.print_exception(exc, val, tb)

        # Tenta salvar em arquivo de log
        try:
            with open("fatal.log", "a", encoding="utf-8") as f:
                traceback.print_exception(exc, val, tb, file=f)
        except:
            pass

    # ------- Gerenciamento de fases e estados -------
    def current_phase(self):
        # Retorna o dicion√°rio da fase atual
        return self.phases[self.phase_idx]

    def current_target(self):
        # Retorna o gesto alvo atual (pr√≥ximo gesto que o usu√°rio deve fazer)
        seq = self.current_phase()["sequence"]
        if self.step_idx < len(seq):
            return seq[self.step_idx]
        return None

    def update_phase_labels(self):
        # Atualiza todos os labels da interface com informa√ß√µes da fase/gesto atual
        p = self.current_phase()

        # Atualiza t√≠tulo
        self.lbl_title.config(text=p["title"])

        # Atualiza status conforme o estado atual
        tgt = self.current_target()
        if self.state == "DETECT":
            self.lbl_status.config(text=f"Fa√ßa o gesto: {tgt}" if tgt else "Fase conclu√≠da.")
        elif self.state == "INTRO":
            self.lbl_status.config(text="Introdu√ß√£o da fase")
        else:
            self.lbl_status.config(text="")
        
        # Atualiza frase em portugu√™s
        self.lbl_phrase.config(text=p.get("phrase", ""))

        # Atualiza lista de gestos (para tela DONE)
        seq_str = "  ¬∑  ".join(p["sequence"])
        self.lbl_done_sub.config(text=f"Voc√™ executou: {seq_str}")

        # Atualiza visibilidade dos bot√µes
        self.update_button_visibility()

    def update_button_visibility(self):
        # Controla a visibilidade do bot√£o "Fase anterior" - bot√£o s√≥ aparece quando n√£o est√° na 1¬™ fase
        if self.phase_idx == 0:
            # Primeira fase: esconde o bot√£o
            self.btn_prev_phase.pack_forget()
        else:
            # Outras fases: mostra o bot√£o
            self.btn_prev_phase.pack(side="left", padx=4)

    def reset_phase(self):
        # Reinicia a fase atual do zero - limpa todos os buffers e contadores, e volta para a tela de introdu√ß√£o (v√≠deo demonstrativo) da mesma fase
        self.step_idx = 0           # Volta para o primeiro gesto da fase
        self.target_streak = 0      # Zera contador de acertos consecutivos
        self.smoother.clear()       # Limpa buffer do suavizador temporal
        self.seq_buf.clear()        # Limpa buffer de sequ√™ncia de frames
        self.show_state("INTRO")    # Volta para tela de introdu√ß√£o
    
    # ---------- Renderiza√ß√£o de texto unicode ------------
    @staticmethod
    def _pick_font_path():
        # Busca uma fonte TrueType instalada no sistema operacional

        # Lista de fontes candidatas em diferentes sistemas
        candidates = [
            "C:/Windows/Fonts/arial.ttf",                           # Windows - Arial
            "C:/Windows/Fonts/seguiemj.ttf",                        # Windows - Segoe UI Emoji
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",     # Linux - DejaVu
            "/Library/Fonts/Arial Unicode.ttf",                     # macOS - Arial Unicode
        ]

        # Retorna o primeiro caminho que existir
        for p in candidates:
            if os.path.exists(p):
                return p
        return None

    @staticmethod
    def draw_text_unicode(img_bgr, text, org, font_size=32, color=(255,255,255)):
        # Desenha texto Unicode em uma imagem usando Pillow

        # OpenCV n√£o suporta bem caracteres Unicode (acentos, emojis, etc). Esta fun√ß√£o usa Pillow (PIL) para renderizar texto com suporte completo a Unicode e fontes TrueType

        # Busca fonte TrueType no sistema
        font_path = LibrasFasesGUI._pick_font_path()

        # Carrega fonte (ou usa padr√£o se n√£o encontrar)
        font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()

        # Pillow trabalha com RGB, ent√£o convertemos
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img_rgb)
        draw = ImageDraw.Draw(pil_img)

        # Converte cor de BGR para RGB (Pillow usa RGB)
        rgb = (int(color[2]), int(color[1]), int(color[0]))

        # Desenha o texto
        draw.text(org, text, font=font, fill=rgb)

        # Converte de volta para BGR (formato OpenCV)
        return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)

    # ------- Navega√ß√£o entre fases -------
    def prev_phase(self):
        # Volta para a fase anterior - s√≥ funciona se n√£o estiver na 1¬™
        if self.phase_idx > 0:
            self.phase_idx -= 1
            self.reset_phase()

    def next_phase(self):
        # Avan√ßa para a pr√≥xima fase ou finaliza o curso - ae estiver na √∫ltima fase, mostra a tela de conclus√£o do curso (FINAL)

        if self.phase_idx >= len(self.phases) - 1:
            # √öltima fase conclu√≠da - mostra tela final
            self.show_state("FINAL")
        else:
            # Avan√ßa para pr√≥xima fase
            self.phase_idx += 1
            self.reset_phase()

    # ------- Gerenciamento de estados da aplica√ß√£o -------
    def show_state(self, new_state):
        # Gerencia a transi√ß√£o entre estados da aplica√ß√£o - ao trocar de estado, libera recursos do estado anterior e inicializa recursos do novo estado.

        # ---- Limpeza do estado anterior ----
        if self.state == "DETECT":
            self.stop_camera()      # Libera webcam
        if self.state == "INTRO":
            self.stop_tutorial()    # Para reprodu√ß√£o do v√≠deo

        # ---- Atualiza√ß√£o para novo estado ----
        self.state = new_state
        self.update_phase_labels()

        # ---- Inicializa√ß√£o espec√≠fica de cada estado ----
        if new_state == "INTRO":
            self.start_tutorial()           # Inicia reprodu√ß√£o do v√≠deo
            self.frame_intro.tkraise()      # Mostra tela de introdu√ß√£o

        elif new_state == "DETECT":
            self.start_camera()             # Abre webcam e MediaPipe
            self.frame_detect.tkraise()     # Mostra tela de detec√ß√£o
            print("[STATE] DETECT", flush=True)
        
        elif new_state == "DONE":
            self.frame_done.tkraise()       # Mostra tela de conclus√£o
            print("[STATE] DONE", flush=True)
        
        elif new_state == "FINAL":
            self.frame_final.tkraise()      # Mostra tela final
            self.lbl_title.config(text="Curso Conclu√≠do")
            self.lbl_status.config(text="")
            print("[STATE] FINAL - Curso completo!", flush=True)

    def start_detect(self):
        # Inicia o modo de detec√ß√£o
        self.show_state("DETECT")

    def back_to_intro(self):
        # Volta para a tela de introdu√ß√£o (v√≠deo demonstrativo)
        self.show_state("INTRO")

    # ------- Gerenciamento do v√≠deo tutorial -------
    def start_tutorial(self):
        # Inicia a reprodu√ß√£o do v√≠deo tutorial da fase atual - tentar abrir o arquivo de v√≠deo especificado na configura√ß√£o da fase
        path = self.current_phase().get("video")
        self.tutorial_path = path
        
        if path:
            try:
                # Tenta abrir com FFMPEG primeiro
                cap = cv2.VideoCapture(path, cv2.CAP_FFMPEG)
                if not cap.isOpened():
                    # Fallback: tenta com codec padr√£o
                    cap = cv2.VideoCapture(path)
                if not cap.isOpened():
                    raise RuntimeError(f"N√£o foi poss√≠vel abrir o v√≠deo: {path}")
                
                self.tutorial_cap = cap
                self.tutorial_running = True
                print(f"[TUTORIAL] aberto: {path}", flush=True)
            
            except Exception as e:
                print("[!] Erro no v√≠deo da fase:", e, flush=True)
                self.tutorial_cap = None
                self.tutorial_running = False
       
        else:
            # Fase sem v√≠deo configurado
            self.tutorial_cap = None
            self.tutorial_running = False

    def stop_tutorial(self):
        # Para a reprodu√ß√£o do v√≠deo tutorial e libera recursos
        self.tutorial_running = False
        try:
            if self.tutorial_cap:
                self.tutorial_cap.release()
        except Exception:
            pass
        self.tutorial_cap = None

    def tutorial_loop(self):
        # Loop de reprodu√ß√£o do v√≠deo tutorial - l√™ frames do v√≠deo e exibe na interface. Quando o v√≠deo termina, reinicia automaticamente do in√≠cio

        # Caso n√£o tenha v√≠deo dispon√≠vel
        if not self.tutorial_running or self.tutorial_cap is None:
            # Cria imagem preta com mensagem
            img = np.zeros((506, 900, 3), dtype=np.uint8)
            cv2.putText(img, "Sem v√≠deo desta fase.", (20, 260),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2, cv2.LINE_AA)
            self._render_on_label(self.preview_label, img)
            return

        # L√™ pr√≥ximo frame do v√≠deo
        ret, frame = self.tutorial_cap.read()

        # Se chegou ao fim do v√≠deo ou erro de leitura
        if not ret or frame is None:
            # Reinicia v√≠deo do in√≠cio (loop)
            self.tutorial_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            ret, frame = self.tutorial_cap.read()

            # Se ainda assim falhar, para o tutorial
            if not ret or frame is None:
                self.tutorial_running = False
                return

        # Redimensiona frame para largura alvo mantendo propor√ß√£o
        disp = self._letterbox(frame, target_w=900)

        # Renderiza na interface
        self._render_on_label(self.preview_label, disp)

    # ------- Gerencimento da c√¢mera e MediaPipe -------
    def start_camera(self):
        # Inicializa a webcam e o MediaPipe Holistic para detec√ß√£o
        
        global mp_holistic

        # ---- Inicializa√ß√£o do MediaPipe (apenas na primeira vez) ----
        if mp_holistic is None:
            import mediapipe as mp
            mp_holistic = mp.solutions.holistic

        # ---- Cria√ß√£o da inst√¢ncia Holistic ----
        if self.holistic is None:
            try:
                self.holistic = mp_holistic.Holistic(
                    static_image_mode=False,        # Modo v√≠deo (n√£o imagens est√°ticas)
                    model_complexity=1,              # Complexidade m√©dia (0=leve, 2=pesado)
                    enable_segmentation=False,       # Desabilita segmenta√ß√£o (n√£o usamos)
                    refine_face_landmarks=False,     # N√£o refina pontos faciais (economiza recursos)
                    min_detection_confidence=0.5,    # Confian√ßa m√≠nima para detectar pessoa
                    min_tracking_confidence=0.5      # Confian√ßa m√≠nima para rastrear entre frames
                )
                print("[MP] Holistic criado", flush=True)
            except Exception as e:
                traceback.print_exc()
                self.lbl_status.config(text=f"Falha ao iniciar MediaPipe: {e}")
                return

        # ---- Abertura da webcam ----
        print("[CAM] abrindo webcam (MSMF)...", flush=True)
        try:
            # Tenta abrir com MSMF (Microsoft Media Foundation - mais est√°vel no Windows)
            self.cap = cv2.VideoCapture(0, cv2.CAP_MSMF)

            # Se MSMF falhar, tenta DSHOW (DirectShow - fallback)
            if (not self.cap) or (not self.cap.isOpened()):
                print("[CAM] MSMF falhou, tentando DSHOW...", flush=True)
                self.cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

            # Se ambos falharem
            if (not self.cap) or (not self.cap.isOpened()):
                self.lbl_status.config(text="N√£o foi poss√≠vel abrir a webcam.")
                self.cap = None
                print("[CAM] webcam N√ÉO abriu", flush=True)
            else:
                print("[CAM] webcam aberta", flush=True)
        except Exception as e:
            print(f"[ERROR] Exce√ß√£o ao abrir webcam: {e}", flush=True)
            traceback.print_exc()

    def stop_camera(self):
        # Libera a webcam e seus recursos
        try:
            if self.cap:
                self.cap.release()
        except Exception:
            pass
        self.cap = None

    # ------- Loops principais -------
    def main_loop(self):
        # Loop principal da aplica√ß√£o (roda a cada 10ms)

        # Delega para o loop apropriado conforme o estado:
        # - INTRO: tutorial_loop() - reproduz v√≠deo
        # - DETECT: detect_loop() - processa webcam e faz predi√ß√µes
        # - DONE/FINAL: n√£o faz nada (telas est√°ticas)
        
        # Usa self.after() para agendamento n√£o-bloqueante compat√≠vel com Tkinter

        if self.state == "INTRO":
            self.tutorial_loop()
        elif self.state == "DETECT":
            self.detect_loop()

        # Agenda pr√≥xima execu√ß√£o em 10ms (~100 FPS m√°ximo)
        self.after(10, self.main_loop)

    def detect_loop(self):
        # Loop de detec√ß√£o de gestos em tempo real
        try:
            # ---- Verifica√ß√£o de webcam ativa ----
            if not self.cap:
                return

            # ---- Captura frame da webcam ----
            ret, frame = self.cap.read()
            if not ret:
                self.lbl_status.config(text="Sem v√≠deo da c√¢mera.")
                return

            # ---- Processamento com MediaPipe ----
            # Converte BGR (OpenCV) para RGB (MediaPipe)
            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Desabilita escrita para otimiza√ß√£o (MediaPipe n√£o modifica)
            img_rgb.flags.writeable = False

            try:
                # Processa frame para detectar landmarks
                results = self.holistic.process(img_rgb)
            except Exception as e:
                traceback.print_exc()
                self.lbl_status.config(text=f"Falha no MediaPipe: {e}")
                return

            # Reabilita escrita (vamos desenhar na imagem depois)
            img_rgb.flags.writeable = True


            # ---- Extra√ß√£o de features ----
            # Conta quantos landmarks de m√£os foram detectados
            num_lm = count_landmarks(results)

            # Extrai todas as features (1662 valores)
            feat1662 = extract_features_holistic(results)

            # Reduz para apenas 144 features relevantes
            feat144 = apply_keep_idx_feat(feat1662)

            # Adiciona ao buffer circular (mant√©m √∫ltimos 30 frames)
            self.seq_buf.append(feat144)

            # ---- Vari√°veis para feedback visual ----
            pred_text = "Observando..."  # Texto padr√£o
            color = (0, 200, 0)          # Verde padr√£o (BGR)
            target_lbl = self.current_target()  # Gesto que deve ser feito

            # ---- Detec√ß√£o de condi√ß√µes especiais ----
            # Verifica se h√° m√£os suficientes na imagem
            no_hands = (num_lm < MIN_LANDMARKS)

            # Verifica se a pessoa est√° parada (sem movimento)
            if len(self.seq_buf) >= MOTION_MIN_FRAMES:
                still = (motion_energy_last(self.seq_buf, k=MOTION_MIN_FRAMES) < MOTION_EPS)
            else:
                still = False

            # ---- Nenhuma m√£o vis√≠vel ----
            if no_hands:
                self.smoother.clear()
                self.target_streak = 0
                pred_text = "Entre na c√¢mera"
                color = (0, 200, 255)
                self.lbl_status.config(text="Entre na c√¢mera: posicione ao menos 1 m√£o vis√≠vel.")

            # ---- M√£os vis√≠veis mas paradas ----
            elif still:
                self.smoother.clear()
                self.target_streak = 0
                pred_text = "Parado"
                color = (0, 200, 255)  # Amarelo
                if target_lbl:
                    self.lbl_status.config(text=f"Mova a m√£o para reconhecer '{target_lbl}'.")

            # ---- Movimento detectado + buffer cheio ‚Üí FAZER PREDI√á√ÉO ----
            elif len(self.seq_buf) >= SEQLEN:
                # Prepara sequ√™ncia para o modelo (√∫ltimos 30 frames)
                seq_arr = np.stack(self.seq_buf, axis=0).astype(np.float32)
                x_in = np.expand_dims(seq_arr[-SEQLEN:], axis=0) # Shape: (1, 30, 144)

                # Faz predi√ß√£o com o modelo LSTM
                probs = self.model.predict(x_in, verbose=0)[0]

                # Aplica suaviza√ß√£o temporal (m√©dia das √∫ltimas K predi√ß√µes)
                avg, cls, conf = self.smoother.push(probs)

                # ---- Se h√° um gesto alvo (estamos em uma fase) ----
                if target_lbl is not None:
                    # Encontra √≠ndice do gesto alvo na lista de classes
                    try:
                        target_idx = int(np.where(self.classes == target_lbl)[0][0])
                    except Exception:
                        target_idx = None

                    # Pega probabilidade do gesto alvo
                    p_target = float(avg[target_idx]) if target_idx is not None else 0.0

                    # ---- Valida√ß√£o: gesto est√° sendo feito corretamente? ----
                    if p_target >= TARGET_ACCEPT:
                        # Acertou! Incrementa contador de acertos consecutivos
                        self.target_streak += 1
                    else:
                        # Errou ou confian√ßa baixa, zera contador
                        self.target_streak = 0

                    # ---- Valida√ß√£o final: manteve acerto por tempo suficiente? ----
                    if self.target_streak >= TARGET_STREAK:
                        # SUCESSO! Gesto reconhecido com confian√ßa
                        pred_text = f"{target_lbl}  {p_target*100:.1f}%"
                        color = (0, 255, 0) # Verde
                        self.lbl_status.config(text=f"Boa! Reconhecido: {target_lbl}")

                        # Reseta contadores
                        self.target_streak = 0
                        self.smoother.clear()

                        # Avan√ßa para pr√≥ximo gesto da fase
                        self.step_idx += 1

                        # Verifica se completou todos os gestos da fase
                        if self.step_idx >= len(self.current_phase()["sequence"]):
                            if self.phase_idx >= len(self.phases) - 1:
                                # √öltima fase conclu√≠da ‚Üí tela final
                                self.show_state("FINAL")
                            else:
                                # Fase conclu√≠da ‚Üí tela DONE
                                self.show_state("DONE")
                            return
                        
                        # Atualiza labels para pr√≥ximo gesto
                        self.update_phase_labels()

                    # ---- Gesto ainda n√£o validado (acertos insuficientes) ----
                    else:
                        # Verifica se deve se abster (confian√ßa muito baixa)
                        if should_abstain(avg, conf_thresh=CONF_THRESH):
                            pred_text = "Analisando..."
                            color = (0, 200, 255) # Amarelo
                            if target_lbl:
                                self.lbl_status.config(text=f"Mantenha '{target_lbl}' por um instante.")
                        else:
                            # Mostra qual gesto foi detectado
                            label = str(self.classes[cls])
                            pred_text = f"{label}  {conf*100:.1f}%"
                            color = (0, 255, 0)  # Verde
                            if target_lbl:
                                self.lbl_status.config(text=f"Fa√ßa o gesto: {target_lbl}")

                # ---- Modo livre (sem gesto alvo) ----
                else:
                    if should_abstain(avg, conf_thresh=CONF_THRESH):
                        pred_text = "Aguardando‚Ä¶"
                        color = (0, 200, 255)
                    else:
                        label = str(self.classes[cls])
                        pred_text = f"{label}  {conf*100:.1f}%"
                        color = (0, 255, 0)

            # ---- Coletando frames (buffer ainda n√£o est√° cheio) ----
            else:
                if target_lbl:
                    self.lbl_status.config(text=f"Coletando‚Ä¶ alvo: {target_lbl}")

            
            # ---- Renderiza√ß√£o do feedback visual ----
            
            base = frame.copy()
            h, w = base.shape[:2]

            # ---- Desenha HUD (Head-Up Display) semitransparente ----
            hud = base.copy()
            cv2.rectangle(hud, (10, 10), (w - 10, 60), (0, 0, 0), -1)  # Ret√¢ngulo preto
            alpha = 0.6  # Transpar√™ncia
            frame_hud = cv2.addWeighted(hud, alpha, base, 1 - alpha, 0)

            # ---- Desenha texto principal com suporte Unicode ----
            frame_hud = self.draw_text_unicode(frame_hud, pred_text, (20, 20), font_size=32, color=color)

            # ---- Desenha indicador do gesto alvo (se houver) ----
            if target_lbl:
                text_alvo = f"Alvo: {target_lbl}"

                # Estima tamanho do texto para criar caixa de fundo
                font_path = self._pick_font_path()
                if font_path:
                    from PIL import ImageFont, Image, ImageDraw
                    font = ImageFont.truetype(font_path, 24)

                    # Usa Pillow para medir dimens√µes do texto
                    dummy = Image.new('RGB', (1, 1))
                    draw = ImageDraw.Draw(dummy)
                    bbox = draw.textbbox((0, 0), text_alvo, font=font)
                    text_width = bbox[2] - bbox[0]
                    text_height = bbox[3] - bbox[1]
                else:
                    # Fallback: estimativa manual
                    text_width = len(text_alvo) * 14
                    text_height = 24
                
                # Desenha ret√¢ngulo escuro semitransparente atr√°s do texto
                overlay = frame_hud.copy()
                padding = 8
                cv2.rectangle(overlay, 
                            (20 - padding, 70 - padding), 
                            (20 + text_width + padding, 70 + text_height + padding), 
                            (0, 0, 0), -1)
                frame_hud = cv2.addWeighted(overlay, 0.7, frame_hud, 0.3, 0)
                
                # Desenha o texto do alvo
                frame_hud = self.draw_text_unicode(frame_hud, text_alvo, (20, 60), font_size=24, color=(255,255,255))
            
            # ---- Informa√ß√µes de debug ----
            y_dbg = 120
            scale = 0.6
            dbg_color = (200, 200, 200)

            # Mostra n√∫mero de landmarks detectados
            cv2.putText(frame_hud, f"LM={num_lm} (min {MIN_LANDMARKS})", (20, y_dbg), cv2.FONT_HERSHEY_SIMPLEX, scale, dbg_color, 1, cv2.LINE_AA); y_dbg += 20

            disp = self._letterbox(frame_hud, self.video_w_target)
            self._render_on_label(self.video_label, disp)
        except Exception as e:
            # Captura e registra qualquer exce√ß√£o no loop
            print(f"[ERROR] Exce√ß√£o no detect_loop: {e}", flush=True)
            traceback.print_exc()
            self.lbl_status.config(text=f"Erro na detec√ß√£o: {e}")

    # ------- Utilit√°rios de renderiza√ß√£o -------
    def _letterbox(self, img, target_w):
        # Redimensiona imagem mantendo propor√ß√£o (letterbox) - calcula escala baseada na largura alvo e redimensiona a imagem mantendo a propor√ß√£o original (n√£o distorce).
        h, w = img.shape[:2]
        scale = target_w / float(w)
        new_w = target_w
        new_h = int(round(h * scale))
        return cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)

    def _render_on_label(self, tk_label, bgr_img):
        # Renderiza imagem OpenCV (BGR) em um Label do Tkinter - Tkinter usa PIL/ImageTk para exibir imagens. Esta fun√ß√£o converte de BGR (OpenCV) para RGB (PIL) e atualiza o Label

        # Import tardio para evitar conflitos (ImageTk depende de tkinter inicializado)
        from PIL import Image, ImageTk

        # Converte BGR ‚Üí RGB
        disp_rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)

        # Cria objeto PIL Image
        im = Image.fromarray(disp_rgb)

        # Cria PhotoImage para Tkinter
        imgtk = ImageTk.PhotoImage(image=im)

        # Armazena refer√™ncia (evita garbage collection)
        tk_label.imgtk = imgtk

        # Atualiza Label
        tk_label.configure(image=imgtk)

    # ------- Finaliza√ß√£o e limpeza -------
    def on_close(self):
        # M√©todo chamado ao fechar a aplica√ß√£o
        try:
            self.stop_tutorial()
            self.stop_camera()
            if hasattr(self, "holistic") and self.holistic:
                self.holistic.close()
        except Exception:
            pass
        self.destroy()
    
    def restart_course(self):
        # Reinicia o curso desde a primeira fase
        self.phase_idx = 0
        self.reset_phase()

# ------- Ponto de entrada da aplica√ß√£o -------
if __name__ == "__main__":
    # Ponto de entrada principal do programa - inicializa a aplica√ß√£o Tkinter e inicia o loop de eventos
    print("[MAIN] start", flush=True)

    try:
        # Cria inst√¢ncia da aplica√ß√£o
        app = LibrasFasesGUI()
    except Exception as e:
        # Se falhar na inicializa√ß√£o, registra erro e re-lan√ßa exce√ß√£o
        import traceback, sys
        traceback.print_exc()
        sys.__stderr__.write(f"[TK-INIT] falhou: {e!r}\n")
        raise

    # Inicia loop de eventos do Tkinter (bloqueia at√© fechar janela)
    app.mainloop()
    print("[MAIN] exit ok", flush=True)
